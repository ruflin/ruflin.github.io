<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Elastia | Ruflin]]></title>
  <link href="http://ruflin.com/blog/categories/elastia/atom.xml" rel="self"/>
  <link href="http://ruflin.com/"/>
  <updated>2014-06-12T12:19:24+02:00</updated>
  <id>http://ruflin.com/</id>
  <author>
    <name><![CDATA[@ruflin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Elastica with multiple Elasticsearch Nodes]]></title>
    <link href="http://ruflin.com/2011/11/21/using-elastica-with-multiple-elasticsearch-nodes/"/>
    <updated>2011-11-21T13:05:00+01:00</updated>
    <id>http://ruflin.com/2011/11/21/using-elastica-with-multiple-elasticsearch-nodes</id>
    <content type="html"><![CDATA[<p>Elasticsearch was built with the cloud / multiple distributed servers in mind. It is quite easy to start a <a href="http://www.elasticsearch.org/guide/reference/modules/cluster.html">elasticsearch cluster</a> simply by starting multiple instances of elasticsearch on one server or on multiple servers. Every elasticsearch instance is called <a href="http://www.elasticsearch.org/guide/reference/api/admin-cluster-nodes-info.html">a node</a>. To start multiple instances of elasticsearch on your local machine, just run the following command in the elasticsearch folder twice:</p>


<p><code>sh
./bin/elasticsearch -f
./bin/elasticsearch -f
</code></p>

<p>As you will see, the first node will be started on port 9200, the second instance on port 9201. Elasticsearch automatically discovers the other node and creates a cluster. Elastica can be used to retrieve all node and cluster information. In the following example first the cluster object is retrieved (Elastica_Cluster) from the client and then the <a href="http://www.elasticsearch.org/guide/reference/api/admin-cluster-state.html">cluster state</a> is read out. Then all cluster nodes (Elastica_Node) are retrieved and the name of every node is printed out. Every cluster has at least one node and every node has a specific name.</p>


<p>```php
$client = new Elastica_Client();</p>

<p>// Retrieve a Elastica_Cluster object
$cluster = $client->getCluster();</p>

<p>// Returns the cluster state
$state = $cluster->getState();</p>

<p>// Gets all cluster notes
$nodes = $cluster->getNodes();</p>

<p>foreach ($nodes as $node) {</p>

<pre><code>echo $node-&gt;getName();
</code></pre>

<p>}
```</p>

<h2>Client to multiple servers</h2>




<p>As elasticsearch is a distributed search engine that can be run on multiple servers, it is possible that some servers fail and still, the search works as expected as the data is stored redundantly (replicas). The <a href="http://www.elasticsearch.org/guide/reference/api/admin-indices-create-index.html">number of shards and replicas</a> can be chosen for every single index during creation. Of course, this can also be set with Elastica through the mapping as can be seen in the <a href="https://github.com/ruflin/Elastica/blob/master/test/lib/Elastica/IndexTest.php">Elastica_Index test</a>. More details on this perhaps in a later blog post.</p>




<p>One of the goals of the distributed search index is availability. If one server goes down, search results should still be served. But if the client connects to only the server that just went down, no results are returned anymore. Because of this, Elastica_Client supports multiple servers which are accessed in a round robin algorithm. This is the only and also most basic option at the moment. So if we start a node on port 9200 and port 9201 above, we pass the following arguments to Elastica_Client to access both servers.</p>


<p>```php
$client = new Elastica_Client(array(</p>

<pre><code>'servers' =&gt; array(
    array('host' =&gt; 'localhost', 'port' =&gt; 9200)
    array('host' =&gt; 'localhost', 'port' =&gt; 9201)
)
</code></pre>

<p>));
```</p>

<p>From now on, every request is sent to one of these servers in a round robin type. Instead of localhost, an external server could be used in addition. I'm aware that this is still a quite basic implementation. As probably some of you already realized, this is no safe failover method, as every second request still goes onto the server that is down. One idea here is to give a specific threshold for every server in which the respond time should be and otherwise the query goes to the next server. In addition, it would be useful to store this information on unavailable servers somewhere in order to use it for the next request. Thus, only one client has to wait for the unavailable server. Storing this information is somehow an issue, since Elastica does not have any storage backend.</p>




<h2>Load Distribution</h2>




<p>This client implementation also allows to distribute the load on multiple nodes. As far as I know, Elasticsearch already does this quite well on its own. But it helps if more than one node can answer http requests. Therefore, the method above is really useful if you use more than one elasticsearch node in a cluster to send your request to all servers.</p>




<p>It is planned to enhance this multiple server implementation in the future with additional parameters such as priority for a server and some other ideas. Please feel free to write down your ideas in the comment section or directly create a pull request on github.</p>

]]></content>
  </entry>
  
</feed>
